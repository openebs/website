[{"id":1,"title":"Repeatable OpenEBS Mayastor deployments and benchmarks\r","author":"OPENEBS\r","author_info":"\r","date":"22-03-2021\r","tags":["Mayastor"," Openebs"],"excerpt":"Learn about Repeatable OpenEBS Mayastor deployments and benchmarks\r","content":"\r\n## Introduction\r\n\r\nOpenEBS is one of the most popular Storage-related projects in CNCF, and the newest addition to OpenEBS - Mayastor, is a missing piece that has been absent from the Kubernetes stack for a long time - Kubernetes-native, high performance, distributed Software Defined Storage or what is increasingly called Container Attached Storage (CAS).\r\n\r\nAs the lead developers of OpenEBS Mayastor, we want to be sure our message of an extremely high performing CAS is not only exciting, but also honest and easy to check. We want every interested user to be able to quickly and easily bring OpenEBS Mayastor up, properly tuned and ready for testing with whatever workload the user prefers to try.\r\n\r\nIn order to deliver on that promise, we have started a [“Demo Playground” project, open sourced on Github](https://github.com/mayadata-io/deployment-automation-playground/tree/main/demo-playground).  Contributions and feedback are welcome.\r\n\r\n\r\n## OpenEBS\r\n\r\nOpenEBS is a project with multiple storage engines, with each engine providing the user with different feature sets as well as different usage and performance characteristics. The currently available options can roughly be split into two categories:\r\n\r\n* LocalPV: Excellent for workloads that deal with storage resilience at the application level, creating and managing their own replicas and capable of sustaining the loss of a single or multiple nodes, such as  Cassandra, and requiring very good storage performance, especially latency-wise.\r\n* Replicated storage  (cStor, Jiva) - for workloads that are less performance-sensitive and some of the more advanced storage features such as synchronous data replication, snapshots, clones, thin provisioning of data, high resiliency of data, data consistency, and on-demand increase of capacity or performance.\r\n\r\nAdvanced features come at the cost of higher latency and lower performance, and yet, technology keeps advancing and trying to get the best of both worlds.\r\n\r\n\r\n## OpenEBS Mayastor\r\n\r\nOpenEBS Mayastor delivers on the promise of exciting new technology, utilizing NVMe (not just the disks, but the protocol and standards), NVMEoF, SPDK and io_uring. NVMes inside our servers deliver amazing speeds and latencies, huge numbers of IOPS, and using old SCSI or FC protocols only waste resources introducing overheads. Harnessing SPDK and NVMEoF OpenEBS Mayastor achieves speeds that are close to in-host NVMes, without compromising on workload mobility, resilience, flexibility, and enterprise features.\r\n\r\nStill, all this exciting tech needs some proper care before it behaves as it should, and we still have a ways to go before it autotunes and autoconfigures itself just right with the help of Kubernetes and workload operators; and yet, as a user willing to take Mayastor for a spin, there should be no reason to wait, if the tuning and preparation can be automated now.\r\n\r\n\r\n## Introducing: the Automation Playground\r\n\r\nThe Automation Playground provides an easy onramp for trying out OpenEBS Mayastor in a cloud or self-hosted environment and attempts to keep the installation process correct, standardized, and consistently reproducible, yet both simple and flexible.\r\n\r\nThe Playground utilizes popular and familiar software in order to apply the desired state configuration, as well as following a familiar Jenkins-pipeline-like approach.\r\n\r\nThe entire process is split into stages, with each stage extensible, replaceable and skippable, if need be, and each stage is called from a simple bash script, where each step is a function, easily copied into a CI engine as a pipeline stage.\r\n\r\nThe user experience is as simple as editing a single variables file in order to define the benchmark setup variables and running up.sh. The script will then iterate over the predefined stages, relying on the outcomes of each stage to run the next one\r\n\r\nVariables are used to define such things as the setup name (prefixed in all the provisioned resources), user access credentials, Kubernetes installation types, provisioning details, and of course, OpenEBS Mayastor tuning as well as the benchmark itself. For more details, please see the vars file at https://github.com/mayadata-io/deployment-automation-playground/blob/main/demo-playground/vars\r\n\r\n\r\n## Stages\r\n\r\nEach software lifecycle consists of several stages - provisioning, deployment, operations, and teardown.\r\n\r\nSince we are flexible here, each stage can be skipped if it isn’t required in a given setup.\r\n\r\nWhen running a benchmark on a set of self-hosted bare metal machines, the provisioning stage is not needed.\r\n\r\nIf Kubernetes is already installed, the Kubernetes installation stage can be skipped.\r\n\r\nWhen running the Demo Playground on a host that has direct access to the machines executing the benchmark, the VPN stage can be skipped.\r\n\r\nThe only truly essential stages are node preparation and the actual OpenEBS Mayastor workload playbooks that will be installed.\r\n\r\n\r\n#### Stage 1: Provisioning\r\n\r\nAt this step, we use Terraform to create a separate environment for the benchmark. Currently, the supported provisioning options are Azure and AWS EC2, with GCP support not too far behind. As a reminder, contributions (and feedback) are welcome.\r\n\r\nTerraform is used to create a separate VPC (in EC2) or Resource Group (in Azure), where networking is configured, and VMs are provisioned as per the definitions in the vars file.\r\n\r\nThe nodes provisioned are of three varieties\r\n\r\n* Master nodes (for Kubernetes Masters)\r\n* Worker nodes (Kubernetes workers that will be running the workload - make sure these are powerful enough and include fast networking if you want to be able to stress Mayastor)\r\n* Storage nodes (Kubernetes workers that will be running Mayastor). These instances should have fast local NVMe disks, which means LXs_v2 on Azure, m5d/m5ad/m5dn/i3 on AWS or n1/n2_standard with added Local-SSDs on GCP.\r\n\r\nWhen provisioning is complete, an Ansible inventory file is generated by Terraform, to be used in later stages. The inventory contains all the node IPs split into groups and adjusted for the various Kubernetes installers in use.\r\n\r\nIf the provisioning stage is skipped, the user must provide the inventory.ini file in the workspace directory, with the file containing the [mayastor_clients] (non-storage workers) and [mayastor_storage] (storage nodes) groups.\r\n\r\n#### Stage 2: Start VPN\r\n\r\nThis is a small stage, only required when the host executing Demo Playground is not inside the same subnet as the cluster nodes. The stage starts sshuttle after creating a script in the workspace directory. Sshuttle is described as a “poor man’s VPN” - an easy to use package that will tunnel all traffic for a given subnet through an SSH tunnel to a Bastion host.\r\n\r\nDuring provisioning, the first Kubernetes Master host has designated the Bastion and will be used for this purpose, effectively working as a VPN concentrator for the Demo Playground setup, placing the executor host in the same subnet as the Kubernetes nodes.\r\n\r\n#### Stage 3: Kubernetes setup\r\n\r\nAt this step, the Playground will deploy a pre-configured version of Kubernetes on the hosts as described in the inventory. If Provisioning was skipped, this means that the inventory file will have to be expanded with groups that are pertinent to the Kubernetes deployment in use; otherwise, the inventory generated in the Provisioning stage will contain all the required groups.\r\n\r\nCurrently two installation types are supported with more planned:\r\n\r\n* Kubespray - a well known Ansible based feature rich Kubernetes installer\r\n* K3S - a simplified and downsized Kubernetes distribution, which can be perfect for a small demo setup. This is also installed via Ansible.\r\n\r\nAt the end of the step, the script will extract the KUBECONFIG credentials file from a Master node and place it under workspace/admin.conf. If this stage is skipped, the user will have to extract and add this file manually.\r\n\r\n#### Stage 4: Node preparation\r\n\r\nIn order to run OpenEBS Mayastor as well as other OpenEBS storage engines, some prerequisites need to be applied to the Kubernetes workers, both the storage and client nodes.\r\n\r\nThis includes making sure the iSCSI and NVMeo-TCP client packages are present, installing and enabling the various Linux kernel modules, enabling hugepages, and so on. Some of these settings might require a host restart.\r\n\r\nThe stage is implemented as an Ansible playbook, which allows it to reach into the hosts directly in order to prepare them, performing some actions a Kubernetes pod has limited access to.\r\n\r\nAt this point, we should have a working Kubernetes setup, with the different worker nodes prepared for using Mayastor either as storage hosts or storage clients.\r\n\r\n## Playbooks\r\n\r\nActually, the proper stages end at Node Preparation, and then the playbooks take over.  The vars file contains a PLAYBOOKS variable, which lists all the playbooks the Playground will apply in sequence.\r\n\r\nCurrently, there is one playbook relevant to testing Mayastor - mayastor.yml\r\n\r\nBut the script will attempt to run any playbooks mentioned from the deployments directory one after another.\r\n\r\nThe Mayastor playbook follows the Mayastor installation instructions, creating the Kubernetes manifests and applying them to the setup, so that all the relevant Mayastor pods, DaemonSets, StorageClasses, Pools etc. are created in the Mayastor namespace, PVCs are created and ready to be used by the user’s workload.\r\n\r\nThe Mayastor playbook also contains an optional FIO test, which will create an FIO pod using the first created PVC and run a quick 1-minute benchmark.\r\n\r\n## Conclusion\r\n\r\nThe Demo Playground project is still in very early stages, and we invite everyone to use, contribute and expand upon it. The goal here is to give the user interested in giving OpenEBS Mayastor a try, a ready tool that does the job in an open, honest, consistent, and reproducible manner.\r\n\r\nThe project’s flexibility allows for anyone to add in additional playbooks, which will deploy and run different workloads on top of Mayastor, and we intend to expand upon it, adding some workloads of our own beyond the basic FIO benchmark.\r\n\r\nPlease visit us at https://mayadata.io and give the Demo Playground a spin at https://github.com/mayadata-io/deployment-automation-playground/tree/main/demo-playground.\r\n\r\nYou can also find my colleagues and me spending time on the Kubernetes #OpenEBS slack, or at a [Discord room](https://discord.com/invite/zsFfszM8J2) set up to focus mostly on open source collaboration with Mayastor developers (Rusticians may be especially interested), and on the Data on Kubernetes community where a huge variety of users of Kubernetes for data are sharing their perspectives (https://dok.community/.","slug":"repeatable-openebs-mayastor-deployments-and-benchmarks"},{"id":2,"title":"How are TikTok, Flipkart, KubeSphere, and others using OpenEBS for Local Volumes\r","author":"Kiran Mova\r","author_info":"Contributor and Maintainer OpenEBS projects. Chief Architect MayaData. Kiran leads overall architecture & is responsible for architecting, solution design & customer adoption of OpenEBS.\r","date":"12-03-2021\r","tags":["Localpv"," Openebs"," Flipkart"," TikTok"," Kubernetes"," Mayastor"," Mayadata"],"excerpt":"How are TikTok, Flipkart, KubeSphere, and others using OpenEBS for Local Volumes\r","content":"\r\n**How to select the right local volume for your workloads?**\r\n\r\nWe have recently seen a massive increase in the usage of different flavors of OpenEBS Local PV. We estimate by looking at container pulls for underlying components combined with some call home data for those users of OpenEBS that enable the capturing of metrics that the weekly new deployments of OpenEBS for LocalPV increased by nearly 10x during 2020. This can be attributed to the fact that more and more cloud native Stateful applications are moving into Kubernetes\r\n\r\n![kg-image](https://admin.mayadata.io/content/images/2021/03/Local-PV-Deployment.PNG)\r\n\r\nSome of the prominent users of OpenEBS Local PV include the CNCF, Optoro, ByteDance / TikTok, Flipkart, and many more. You can always read more about OpenEBS users on the OpenEBS.io website and on the GitHub project page here: https://github.com/openebs/openebs/blob/master/ADOPTERS.md.\r\n\r\nWhile Kubernetes provides native support or interfaces for consuming Local Volumes, the adoption of OpenEBS for LocalPV management suggests that some capabilities are missing that are desired by users. At a high level, dynamic provisioning and the simplicity of deleting Local Volumes are two reasons often given for the preference of some users for the use of OpenEBS LocalPV.\r\n\r\nIn this blog, I outline the various types of Local Storage that users have in their Kubernetes clusters and introduce the various flavors of OpenEBS Local PV being used.\r\n\r\nBefore getting into the flavors of OpenEBS Local PV, it might be worthwhile to know what Kubernetes offers or means by a Local Volume.\r\n\r\n*A [Kubernetes Local Volume](https://kubernetes.io/docs/concepts/storage/volumes/#local) implies that storage is available only from a single node. A local volume represents a mounted local storage device such as a disk, partition, or directory.*\r\n\r\nSo, it stands to reason - as the Local Volume is accessible only from a single node, local volumes are subject to the availability of the underlying node. If the node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run.\r\n\r\nHence, Stateful Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.\r\n\r\nAs it happens, many of the Cloud Native Workloads - are distributed in nature and are typically deployed as StatefulSets with multiple replicas. These can sustain the failure or reduced availability of a single replica. MinIO, Redis, PostgreSQL, Kafka, Cassandra, Elastic are just some examples that are deployed using Local Volumes. For these applications - performance and consistent low latency, and ease of management are more important than the resiliency of a node to failures.\r\n\r\nAs the large SaaS provider, [Optoro](https://github.com/openebs/openebs/blob/master/adopters/optoro/README.md) puts it:\r\n*The vast majority of applications are able to better handle failover and replication than a block level device. Instead of introducing another distributed system into an already complex environment, OpenEBS's localPVs allow us to leverage fast local storage. … OpenEBS has allowed us to not introduce a complicated distributed system into our platform. The adoption has been smooth and completely transparent to our end users.*\r\n\r\n## Limitations of Kubernetes LocalPV\r\n\r\nKubernetes expects users to make Persistent Volumes (PVs) available that it can then associate with PVCs during scheduling. Kubernetes does not help with dynamically creating these PVs as the applications are launched into the cluster.\r\n\r\nThis pre-provisioning can become an issue when companies have more than two people or teams managing the Kubernetes clusters, and the Application teams depend on the Kubernetes cluster administrators for provisioning the Volumes.\r\n\r\nWe have seen that cluster administrators are challenged by the following aspects:\r\n\r\n(a) The type of storage available on the Kubernetes nodes varies depending on how the Kubernetes nodes are provisioned. Available storage types include:\r\n\r\n* Nodes have only OS disks with large space that can be used for provisioning Local Volumes.\r\n* Nodes have one or two additional devices (SSDs or Disks) attached that can be used for provisioning Local Volumes.\r\n* Nodes have 8 to 16 high-performing NVMe SSDs.\r\n\r\n(b) And then, there is a matter of capacity available from the Local Storage and how to manage this to enable the freedom of developers and other consumers of capacity while retaining a level of oversight and assistance by centralized teams:\r\n\r\n(c) First, the platform or other centralized team may not know exactly what the capacity a particular team or workload needs - and the developer or data scientist may not know either. Dynamic provisioning within quotas means that users can keep moving without opening a ticket or having a conversation.\r\n\r\n(d) Secondly, there are many common operations tasks that need to be performed. Just because the applications are resilient does not mean these tasks entirely disappear. Administrators still would like to safeguard the data with best practices from years of experience in dealing with data such as:\r\n\r\n* Enforcing Capacity Limits/Thresholds\r\n* Securing the Volumes\r\n* Carving out the Local Volumes from well known or familiar file systems like LVM, ZFS, XFS, and so forth\r\n* Encrypting the Volumes\r\n* Enforce compliance with BCP by taking regular snapshots and full backups\r\n\r\nThis is where Kubernetes itself stops, and plugins like OpenEBS LocalPV options step into the auto-magically provision and manage the Local Volumes.\r\n\r\n## Selecting your LocalPV\r\n\r\nOpenEBS provides different types of Local Volumes that can be used to provide locally mounted storage to Kubernetes stateful workloads. The choice of the OpenEBS Local Volume depends on the type of local storage available on the node and the features required.\r\n\r\n* OpenEBS Hostpath Local PV - The use of the host path is the simplest, most used, and lowest overhead solution. This approach creates Local PVs by creating a sub-directory per Persistent Volume. This offers flexibility to create different classes of storage and allows administrators to decide into which parent or mounted directory the Persistent Volumes sub-directories should be placed. For example - a storage class for critical workloads vs. non-critical transient workloads, SSD vs. Hard Disk mounted paths, and so forth.\r\n* OpenEBS Raw file Local PV - The OpenEBS Raw file approach evolved out of the Hostpath approach due to considerable feedback from some OpenEBS community members. Raw file Local PV offers all the benefits of Hostpath Local PV - and in addition, Hostpath supports enforcing Capacity Quotas on Volume subdirectories by creating sparse files per volume.\r\n* OpenEBS Device Local PV - Device Local PV is best suited for cases where either a complete device or a partitioned device needs to be dedicated to the pod. Workloads like Cassandra or Kafka that need high throughput and low latency often use dedicated device Local PV.\r\n* OpenEBS ZFS and LVM Local PV - Both ZFS and LVM are selected by seasoned storage administrators that want to leverage all the good things of well-known filesystems or volume management along with the power of Local Volumes. This category offers features like full/incremental snapshots, encryption, thin-provisioning, resiliency against local disk failures by using software raid/mirror, and so forth. Incidentally, you can easily cause a fairly reasoned argument by asking users and community members, and even our own engineers to share their opinions about whether ZFS or LVM is more useful; I'm very happy that the community has progressed to the point that both solutions are now supported and widely deployed.\r\n\r\nI hope this overview of LocalPV options and OpenEBS Local has been useful. I plan to follow this with further blogs that get into the details of each flavor of the OpenEBS Local PV.\r\n\r\nIn the meantime, you can get started easily with [OpenEBS Local PV](https://docs.openebs.io/docs/next/overview.html), and the community is always available on the Kubernetes Slack #openebs channel.\r\n\r\nOr read more on what our OpenEBS users and partners have to say about Local PV. From our friends at 2nd Quadrant (now part of EDB): [Local Persistent Volumes and PostgreSQL usage in Kubernetes](https://www.2ndquadrant.com/en/blog/local-persistent-volumes-and-postgresql-usage-in-kubernetes/)\r\n\r\nAnd from one of the most broadly deployed Kubernetes distributions, Kubesphere: [OpenEBS Local PV is default Storage Class in Kubesphere](https://github.com/openebs/openebs/tree/master/adopters/kubesphere)\r\n\r\nOr, again, you can find more stories and can add your own to Adopters.MD on the OpenEBS GitHub: https://github.com/openebs/openebs/blob/master/ADOPTERS.md","slug":"how-are-tiktok-flipkart-kubesphere-and-others-using-openebs-for-local-volumes"},{"id":3,"title":"OpenEBS NDM, go-to solution for managing Kubernetes Local Storage\r","author":"Akhil Mohan\r","author_info":"Software Engineer @ MayaData, working on Cloud Native Tech.\r","date":"13-01-2021\r","tags":["openebs"],"excerpt":"Read about OpEBS NDM, the go-to solution for managing Kubernetes Local Storage.\r","content":"\r\nEver since Local Volumes have become generally available (GA) in Kubernetes 1.14, the use of Local Volumes has skyrocketed. This can be attributed to the nature of cloud-native workloads distributed in nature and can sustain node failures. The bare metal underpinning Kubernetes clusters, both on-prem and cloud, can now be configured with local storage to manage stateful workloads. Kubernetes doesn’t treat storage like a native resource on par with CPU or Memory, making it a little difficult to make Kubernetes work out of the box to create effective node-attached storage. OpenEBS NDM helps alleviate this gap by discovering the different storage types attached to each worker node and then creating Kubernetes resources called block devices.\r\n\r\nApplication or storage operators can then use the information exposed via block devices to determine how to orchestrate the workloads best.\r\n\r\nOpenEBS NDM (Node Device Manager) has been declared GA after being deployed in production for several months as part of the OpenEBS control plane. With the release of version 1.0, NDM adds out-of-the-box support for partitions, LVMs, LUKS encrypted devices, in addition to the unique identification of virtual disks within the cluster. Now offering support for partitions, a single disk can be partitioned. Each partition will be considered a separate block device used by different storage engines like cStor / local PV. NDM also tracks the movement of the devices within a cluster across the nodes.\r\n\r\n## Key Storage Problems solved by NDM\r\n\r\n* Local Storage Discovery - detecting partitions, devices used as a LUKS device or LVM device, or if it can be accessed as a raw block device.\r\n* Cluster-wide storage visibility\r\n* Detect the movement of storage devices across nodes\r\n* Book-keeping/storage asset management  - allocating/reserving, which type of storage should be provided to which workloads.\r\n\r\n## Getting Started with NDM\r\n\r\nLet us see how NDM helps detect the block devices in the cluster with 3 nodes, each having a completely different disk configuration. The Disk configuration of the nodes are as follows:\r\n\r\nMaster: 2 virtual disks\r\n\r\nWorker1: 3 virtual disks, one being used by LUKS and two other disks which are partitioned, several partitions are being used as PV's by the LVM.\r\n\r\nWorker 2: 4 physical disks\r\n\r\n* Deploy NDM into the Kubernetes cluster along with OpenEBS LocalPV\r\n\r\n```kubectl apply -f https://openebs.github.io/charts/openebs-operator-lite.yaml```\r\n\r\n(The latest helm charts for deploying NDM are available [here](https://openebs.github.io/node-disk-manager/))\r\n\r\n* Once deployed, check the blockdevices present in the cluster using\r\n\r\n```kubectl get bd -n openebs -o wide```\r\n\r\nSome block devices show partitions that did not exist initially. E.g., sdb1 instead of sdb. This is because NDM creates a partition on virtual disks to identify the disk uniquely. Also, block device resources are now created for LVMs and LUKS encrypted devices. All the block devices listed above will now be treated as individual devices and can be used by any storage engine.\r\n\r\n* Deploy a sample application to use the block device\r\n\r\nDownload the minio yaml and apply it. (NOTE: A node selector has been added to the minio application pod so that it gets scheduled on worker-1)\r\n\r\nkubectl apply -f [minio-official.yaml](https://gist.githubusercontent.com/akhilerm/194a1606c514d8930addcaef56f9f19f/raw/7d339e5042b4e5e958dde558f1f3509e26c214f3/minio-official.yaml)\r\n\r\nNow check the status of block devices again\r\n\r\nWe can see that the device `dm-2`, is the LUKS device, has been claimed and used by the application.\r\n\r\n* Pool movement across nodes\r\n\r\nNDM helps in seamlessly moving cStor pools from one node to another. Whenever the devices that constitute a pool are moved from one node to another (disconnecting disks from one node and reconnecting on another), the block device resource is updated with the latest information. NDM tracks this movement. cStor can use this information to migrate pools as required.\r\n\r\n* Reserving storage for workloads\r\n\r\nNDM provides a feature to reserve devices for certain workloads. E.g., Users can reserve all SSDs for a performance intensive workload. This reservation is achieved using block-device-tags. More information on using block-device-tags with LocalPV can be found [here](https://docs.openebs.io/docs/next/uglocalpv-device.html#optional-block-device-tagging).\r\n\r\n## Future Roadmap\r\n\r\n* Southbound provisioning\r\n* Metrics (currently in alpha)\r\n* API Service to interact with NDM\r\n* Ability to create partitions or LVM volume groups - preparing storage in general.\r\n\r\n## Interested in Contributing to NDM?\r\n\r\nNDM is an OpenEBS project, which itself is a CNCF sandbox project. [OpenEBS on GitHub](https://github.com/openebs/node-disk-manager) is a great place to join if you want to contribute to our codebase. You can also interact with us on the OpenEBS channel in [Kubernetes Slack](https://kubernetes.slack.com/?redir=%2Fmessages%2Fopenebs%2F).","slug":"openebs-ndm-goto-solution-for-managing-kubernetes-local-storage"},{"id":4,"title":"Storage is Evolving!\r","author":"Nick Connolly\r","author_info":"Nick is the Chief Scientist at MayaData and a pioneer of storage virtualization, holding patents ranging from highly-scalable algorithms through to data protection techniques.\r","date":"11-12-2020\r","tags":["openebs"],"excerpt":"Learn how storage has evolved over the years. \r","content":"\r\nBefore the turn of the century, storage systems were typically controlled by dedicated firmware running on custom hardware. These proprietary systems were time-consuming to design, expensive to build, and resistant to innovation.\r\n\r\nIn 1998, Software Defined Storage was pioneered by DataCore Software with its SANsymphony suite of products, based on the realization that general-purpose computers had become fast enough to handle the demands of a high-performance storage stack. For context, this was an era when a system with more than two cores was a rarity and both memory and storage were measured in MBs! The primary protocol in use in the enterprise was SCSI, whether directly connected or accessed through a Fibre Channel network, response times were measured in the tens of milliseconds, and accessing storage over Ethernet using iSCSI was only just starting to be worked on.\r\n\r\n## The hardware environment is changing!\r\n\r\nIn the last few years, the hardware environment has changed significantly. Instead of the relentless drive for ever-increasing clock speeds, systems with over a hundred cores are now mainstream. Developing highly-performant algorithms that operate at this scale of parallelism is a complex and time-consuming process that, generally speaking, is uneconomic to pursue.  Storage media has also undergone a transformation, with SSDs based on flash memory delivering orders of magnitude better performance than spinning disks. Their response time, which can be measured in microseconds, has highlighted the inefficiencies of the decades-old SCSI protocol.\r\n\r\nNVMe is a ‘state of the art’ storage protocol for a new era. Designed from the ground up for maximum parallelism and lock-free operation, it offers up to 64k independent I/O queues each with 64k entries and a simplified command set. Connected over PCIe, it delivers low latency and high bandwidth data directly to an application, enabling it to fully utilize the capabilities of the underlying flash memory. NVMe over Fabrics (NVMe-oF) provides network access to remote storage and targets less than 10 microseconds in additional latency.\r\n\r\n## Application development is changing!\r\n\r\nRather than building the large monolithic codebases that were the norm at the turn of the century, modern development practices are based around composable architectures; containerized microservices that scale dynamically to meet performance requirements. For more background on this trend, see my [earlier post](https://www.datacore.com/blog/5-changes-that-are-reshaping-software-development/) and the excellent articles in [MayaData’s blog](https://blog.mayadata.io/). Kubernetes is rapidly becoming the control plane for the enterprise.\r\n\r\n## A New Era\r\n\r\nA new era requires a new kind of storage stack! A stack that is based around today’s technologies rather than being anchored to the last century. A stack that is portable and flexible. A stack that supports rapid innovation. That delivers the performance that applications require.\r\n\r\n## Container Attached Storage\r\n\r\nThe new category of [Container Attached Storage](https://www.cncf.io/blog/2018/04/19/container-attached-storage-a-primer/), of which OpenEBS is the de-facto open source standard, orchestrates the storage stack with the same flexibility as the application.  Implemented as a microservices based architecture, it runs within Kubernetes and gives users the freedom to define the way that they want to access, protect, and manage their data. The days of the dedicated storage administrator are coming to an end!\r\n\r\nFor Mayastor, the latest storage engine to be added to OpenEBS, flexibility, and performance are achieved by basing the stack around the [Storage Platform Development Kit (SPDK)](https://spdk.io/), which provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications. Based on the NVMe protocol, it delivers blistering performance from today’s hardware as well as being ready for the next generation of Intel Optane based SSDs that are just becoming available. For more details, see some [recent results](https://openebs.io/blog/mayastor-nvme-of-tcp-performance/).\r\n\r\n## Microsoft Windows\r\n\r\nHowever, amid all the discussions about flexibility and portability, there is one small footnote that often goes unnoticed: ‘not supported on Windows’. It’s understandable, because most of the projects that are shaping this new era have their roots on Linux or FreeBSD, but it overlooks the sheer scale of Windows Server deployments in enterprise environments. Things are changing, with significant investments being made in Kubernetes on Windows, but it’s a slow process; one project at a time!\r\n\r\nMayaData’s mission is to enable data agility - so we were uncomfortable with our high-performance Container Attached Storage solution, OpenEBS Mayastor, not being available on Windows platforms. With that in mind, we have created the [Windows Platform Development Kit (WPDK)](https://github.com/wpdk/wpdk) to act as a foundational layer to make it easier to port the SPDK to Windows. In addition, we are working with the SPDK community to make a few changes to the code base to support this.  It is a testament to the quality of the excellent SPDK project that so few changes have been required so far.\r\n\r\nThe project also benefits from the work done by the DPDK on Windows community who has invested a significant amount of time porting the underlying [Data Plane Development Kit (DPDK)](https://www.dpdk.org/), a Linux Foundation project that consists of libraries to accelerate packet processing workloads running on a wide variety of CPU architectures.\r\n\r\n## Windows Platform Development Kit\r\n\r\nThe MayaData developed and contributed Windows Platform Development Kit has currently reached ‘alpha’. Most of the required functionality is believed to be present, unit tested, and working correctly, but there are still areas that need further development.\r\n\r\nIt is possible to build the SPDK tree, run the associated unit tests, serve an iSCSI target on Windows, and mount it as a volume.\r\n\r\nIt is anticipated that this collaboration will deliver the following benefits to Windows users:\r\n\r\n1. Enable high-performance access to NVMe storage directly from applications.\r\n2. Native software defined storage stacks, including OpenEBS Mayastor.\r\n3. Support for NVMe-oF adaptors from manufacturers such as Mellanox and Broadcom.\r\n\r\nThe Windows Platform Development Kit is open source, under a BSD-3 clause license.  Community contributions are welcomed and needed! To get started please head to https://wpdk.github.io or access the WPDK code and documentation on [GitHub](https://github.com/wpdk/wpdk).","slug":"storage-is-evolving"},{"id":5,"title":"OpenEBS on DigitalOcean Marketplace\r","author":"Abhishek\r","author_info":"Abhishek is a Customer Success Engineer at Mayadata. He is currently working with Kubernetes and Docker.\r","date":"3-12-2020\r","tags":["openebs"," chaosengineering"," tutorials"],"excerpt":"Learn how to deploy OpenEBS on the DigitalOcean marketplace\r","content":"\r\nDeploying OpenEBS on DigitalOcean can directly be done from the console. DigitalOcean provides the feature to create a cluster with OpenEBS deployed on it already. To get started, follow the below-mentioned steps:\r\n\r\nWORKFLOW:\r\n\r\nSTEP 1: Getting started\r\nLogin to your [DigitalOcean](https://cloud.digitalocean.com/login) account.\r\n\r\nSTEP 2: Creation of cluster\r\nOnce you log in, you arrive at the dashboard, click on Marketplace under DISCOVER located on the left sidebar.\r\n\r\nNext, scroll down to find OpenEBS. On clicking, you will be redirected to a page where you will find the details about OpenEBS and the Create OpenEBS button on the right side.\r\n\r\nNext, you need to provide the necessary details such as Data Center region, cluster capacity, cluster name, etc. (It is advisable to provision 3 nodes with 4vCPUs and 8 GB memory to ensure that the resources are sufficient at all times.)\r\n\r\nSTEP 3: Connecting your cluster\r\nCreation, resizing, and deletion can be carried out from UI, but you require command-line tools from your local machine or a remote management server to perform administrative tasks. The detailed steps to install the management tools and connect the cluster to your local machine can be found under the Overview section.\r\n\r\nTo verify, execute the following command:\r\n\r\n```$ kubectl get ns```\r\n\r\nOutput:\r\n```\r\nNAME     STATUS    AGE\r\ndefault  Active    13m\r\nopenebs  Active    13m\r\n```\r\nThe output must contain openebs ns in an Active state.\r\n\r\nNext, execute:\r\n\r\n```$ kubectl get pods -n openebs```\r\n\r\nOutput:\r\n```\r\nNAME                                                 READY     STATUS    RESTARTS AGE\r\nopenebs-admission-server-5c4d545647-r4vgr            1/1       Running   0        13m\r\nopenebs-apiserver-56f77c9965-xft68                   1/1       Running   2        13m\r\nopenebs-localpv-provisioner-64c67b5b89-czv8b         1/1       Running   0        13m\r\nopenebs-ndm-5f6nt                                    1/1       Running   0        13m\r\nopenebs-ndm-74njq                                    1/1       Running   0        13m\r\nopenebs-ndm-operator-7bc769dcff-b45bc                1/1       Running   1        13m\r\nopenebs-ndm-spttv                                    1/1       Running   0        13m\r\nopenebs-provisioner-755f465f9d-fr67l                 1/1       Running   0        13m\r\nopenebs-snapshot-operator-7988fc8646-zpd98           2/2       Running   0        13m\r\n```\r\nAll the pods must be in a running state.\r\n\r\nSTEP 4: Attaching BlockDevices\r\nTo attach BlockDevices to the created nodes, click on Volumes on the left sidebar and then click on the Add Volume button.\r\n\r\nNext, select the volume size ( provision at least 30 GB), select the node(droplet) to which it gets attached and provides a name, then click on the Create Volume button. Repeat these steps for each node (droplet).\r\n\r\nNOTE:\r\n\r\n*For cStor, choose Manually Mount and Format under Choose Configuration Options.*\r\n\r\n*For Jiva, choose Automatically Format and Mount under Choose Configuration Options.*\r\n\r\n*After the BlockDevices get attached for all the nodes, you can see an output similar to the below image.*\r\n\r\nNext, you have to provision OpenEBS volumes.  Click [here](https://docs.openebs.io/docs/next/ugcstor.html#provisioning-a-cStor-volume) to know more.","slug":"openebs-on-digitalocean-marketplace"},{"id":6,"title":"Atlassian Jira Deployment on OpenEBS\r","author":"Abhishek\r","author_info":"Abhishek is a Customer Success Engineer at Mayadata. He is currently working with Kubernetes and Docker.\r","date":"20-11-2020\r","tags":["openebs"],"excerpt":"Learn how to deploy Atlassian Jira on OpenEBS in this short post.\r","content":"\r\n**Jira** Software is part of a family of products designed to help teams of all types manage work. Originally, **Jira** was designed as a bug and issue tracker. But today, Jira has evolved into a powerful work management tool for all kinds of use cases, from requirements and test case management to agile software development.\r\n\r\n## Requirements\r\n\r\n#### Install OpenEBS\r\n\r\nIf OpenEBS is not installed in your K8s cluster, this can be done from [here](https://docs.openebs.io/docs/next/installation.html). If OpenEBS is already installed, go to the next step.\r\n\r\n#### Configure cStor Pool\r\n\r\nIf cStor Pool is not configured in your OpenEBS cluster, this can be done from [here](https://docs.openebs.io/docs/next/ugcstor.html#creating-cStor-storage-pools). Sample YAML named **openebs-config.yaml** for configuring cStor Pool is provided:\r\n\r\n```\r\n#Use the following YAMLs to create a cStor Storage Pool.\r\n# and associated storage class.\r\napiVersion: openebs.io/v1alpha1\r\nkind: StoragePoolClaim\r\nmetadata:\r\n  name: cstor-disk\r\nspec:\r\n  name: cstor-disk\r\n  type: disk\r\n  poolSpec:\r\n    poolType: striped\r\n  # NOTE - Appropriate disks need to be fetched using `kubectl get blockdevices -n openebs`\r\n  #\r\n  # `Block devices` is a custom resource supported by OpenEBS with `node-disk-manager`\r\n  # as the disk operator\r\n# Replace the following with actual disk CRs from your cluster `kubectl get blockdevices -n openebs`\r\n# Uncomment the below lines after updating the actual disk names.\r\n  blockDevices:\r\n    blockDeviceList:\r\n# Replace the following with actual disk CRs from your cluster from `kubectl get blockdevices -n openebs`\r\n#   - blockdevice-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\n#   - blockdevice-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\n#   - blockdevice-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\n\r\n---\r\n```\r\n\r\n#### Create Storage Class\r\n\r\nYou must configure a StorageClass to provision cStor volume on the cStor pool. In this solution, we are using a StorageClass to consume the cStor Pool, which is created using external disks attached to the Nodes. Since Jira is a deployment application, it requires three replications at the storage level. So cStor volume replicaCount is 3. Sample YAML named **openebs-sc-disk.yaml** to consume cStor pool with cStor volume replica count as 3 is provided:\r\n\r\n```\r\napiVersion: storage.k8s.io/v1\r\nkind: StorageClass\r\nmetadata:\r\n  name: openebs-cstor-disk\r\n  annotations:\r\n    openebs.io/cas-type: cstor\r\n    cas.openebs.io/config: |\r\n      - name: StoragePoolClaim\r\n        value: \"cstor-disk\"\r\n      - name: ReplicaCount\r\n        value: \"3\"       \r\nprovisioner: openebs.io/provisioner-iscsi\r\nreclaimPolicy: Delete\r\n```\r\n\r\n### Deployment of Jira\r\n\r\nSample Jira Yaml:\r\n\r\n```\r\napiVersion: extensions/v1beta1\r\nkind: Deployment\r\nmetadata:\r\n  labels:\r\n    app: jira\r\n  name: jira\r\nspec:\r\n  replicas: 1\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: jira\r\n      name: jira\r\n    spec:\r\n      containers:\r\n        - name: jira\r\n          image: \"doriftoshoes/jira:7.3.6\"\r\n          resources:\r\n            requests:\r\n              cpu: \"2\"\r\n              memory: \"2G\"\r\n          volumeMounts:\r\n            - name: \"jira-home\"\r\n              mountPath: /opt/jira-home\r\n      volumes:\r\n        - name: jira-home\r\n          persistentVolumeClaim:\r\n            claimName: demo-vol1-claim\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  labels:\r\n    app: jira\r\n  name: jira\r\nspec:\r\n  ports:\r\n    - port: 8080\r\n      targetPort: 8080\r\n  selector:\r\n    app: jira\r\n  type: LoadBalancer\r\n---\r\nkind: PersistentVolumeClaim\r\napiVersion: v1\r\nmetadata:\r\n  name: demo-vol1-claim\r\nspec:\r\n  storageClassName: openebs-cstor-disk\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 10G\r\n```\r\n\r\nNext, apply both the **Jira deployment** and service to your Kubernetes cluster.\r\n\r\n```kubectl apply -f jira.yaml```\r\n\r\n#### Verify Jira Pods:\r\n\r\n#### Run the following to get the status of Jira pods:\r\n\r\n```kubectl get pods```\r\n\r\nFollowing is an example output:\r\n\r\n```\r\nNAME                    READY   STATUS    RESTARTS   AGE\r\njira-5xxxxxxxx-2xxxx    1/1     Running   0          1d12h\r\n```\r\n\r\nThat's it for today's blog. Thanks for reading. Please leave your questions or feedback, if any, in the comment section below.","slug":"atlassian-jira-deployment-on-openebs"}]